{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "! pip install transformers\n",
    "! pip install unsloth\n",
    "! pip install trl==0.14.0\n",
    "\n",
    "! pip install --upgrade unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import disable_caching\n",
    "disable_caching()\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments,AutoTokenizer,AutoModelForCausalLM, Trainer\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "DATASET_NAME = \"AlgorithmicResearchGroup/ArXivDLInstruct\"\n",
    "SEED = 42\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "SUBSET_SIZE = 1000 # 1000, 1500, 2000, 5000, 10K\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME, split='train')\n",
    "filtered_dataset = dataset.filter(lambda example: len(example[\"function\"]) <= 1000)\n",
    "\n",
    "train_test_split = filtered_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "train_data, test_data = train_test_split[\"train\"], train_test_split[\"test\"]\n",
    "\n",
    "train_data = train_data.shuffle(seed=SEED).select(range(SUBSET_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,  \n",
    "    use_gradient_checkpointing=\"unsloth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True,\n",
    "    random_state=SEED,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def format_prompt(example):\n",
    "    prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction: {example['prompt']}\\n### Response: {example['function']}\" + EOS_TOKEN\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "formatted_train_data = train_data.map(format_prompt, num_proc=None, keep_in_memory=False)\n",
    "\n",
    "# Tokenize the dataset (Add this step)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "formatted_train_data = formatted_train_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns (optional step)\n",
    "formatted_train_data = formatted_train_data.remove_columns(['text', 'full_code', 'function_name', 'description', 'file', 'extension_type', 'function_summary', 'file_number', 'repo', 'file_length', 'avg_line_length', 'max_line_length'])\n",
    "\n",
    "# Create the DataCollator\n",
    "response_template = \"### Response:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_train_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=-1,\n",
    "        num_train_epochs=3,\n",
    "        dataloader_num_workers=0,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        run_name=f\"llama3_finetune_{SUBSET_SIZE}\",\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=3407,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        weight_decay=0.01,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"llama3_finetune_{SUBSET_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(token=os.getenv('HUGGINGFACE_TOKEN'))\n",
    "\n",
    "model.push_to_hub(f\"moosejuice13/llama3_finetune_{SUBSET_SIZE}\")\n",
    "tokenizer.push_to_hub(f\"moosejuice13/llama3_finetune_{SUBSET_SIZE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
